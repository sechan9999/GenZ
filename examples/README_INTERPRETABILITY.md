# Healthcare Model Interpretability Framework

## Overview

This framework demonstrates **how to balance model complexity vs. interpretability** in clinical and policy settings, following the evidence-based rule of thumb:

### üéØ **Decision Framework**

| Use Case | Recommended Model | Interpretability Tools | Deployment Criteria |
|----------|------------------|----------------------|---------------------|
| **Individual Patient Decisions** | Logistic Regression or Monotonic GBM | SHAP waterfall plots + feature importance | Clinician can explain every prediction |
| **Population Surveillance** | XGBoost | SHAP global + PDP + subgroup analysis | Policy-relevant insights, equity verified |
| **Black-Box (if justified)** | Random Forest / Deep Learning | Model cards + monitoring + SHAP | AUC gain >20% + strict governance |

---

## üìä What's Implemented

### **Scenario 1: Individual Patient Decisions**
**Use Case**: Predict 30-day hospital readmission risk to guide discharge planning

**Model**: Logistic Regression (most interpretable)

**Why**:
- Clinicians can understand coefficients as log-odds ratios
- Direct relationship between features and predictions
- Easy to identify harmful biases
- Regulatory compliance (GDPR "right to explanation")

**Outputs**:
- ‚úÖ Logistic regression coefficients (interpretable weights)
- ‚úÖ SHAP waterfall plots for individual patient explanations
- ‚úÖ Feature importance ranked by clinical impact

**Example**:
```python
from healthcare_model_interpretability import ClinicalDecisionModel

# Train interpretable model
model = ClinicalDecisionModel(use_monotonic_gbm=False)
model.train(X_train, y_train)

# Explain prediction for Patient 0
model.explain_prediction(X_test, patient_idx=0)

# Get global feature importance
importance = model.get_feature_importance()
```

**Result**: AUC = 0.797
- Coefficient for `hemoglobin`: -0.69 ‚Üí Lower hemoglobin increases readmission risk
- Coefficient for `comorbidity_count`: +0.68 ‚Üí More comorbidities increases risk
- Coefficient for `age`: +0.66 ‚Üí Older patients at higher risk

---

### **Scenario 2: Population Surveillance**
**Use Case**: Monitor regional readmission trends for healthcare policy decisions

**Model**: XGBoost (acceptable with interpretability tools)

**Why**:
- Population-level predictions allow more complexity
- SHAP can decompose global patterns
- Partial Dependence Plots show marginal effects
- Subgroup analysis ensures equity

**Outputs**:
- ‚úÖ SHAP summary plot (global feature importance)
- ‚úÖ SHAP dependence plots (feature interactions)
- ‚úÖ Partial Dependence Plots (marginal effects)
- ‚úÖ Subgroup performance analysis (age, geography, SES)

**Example**:
```python
from healthcare_model_interpretability import PopulationSurveillanceModel

# Train XGBoost
pop_model = PopulationSurveillanceModel()
pop_model.train(X_train, y_train)

# Generate global interpretability
pop_model.global_interpretability(X_test)

# Analyze fairness across age groups
pop_model.subgroup_analysis(X_test_with_groups, y_test, 'age_group')
```

**Result**: AUC = 0.764
- SHAP identified `age`, `comorbidity_count`, `hemoglobin` as top features
- Partial Dependence Plot shows risk increases sharply after age 70
- Subgroup analysis reveals model performs worst for 80+ age group (AUC=0.62)
  - **Action**: Retrain with oversampling of elderly patients

---

### **Scenario 3: Black-Box Model with Strict Monitoring**
**Use Case**: Deploy complex ensemble if AUC gain > 20% over baseline

**Model**: Random Forest (200 trees, depth=10) as proxy for black-box

**Why**:
- Only justified if predictive gain is massive (>20% relative AUC improvement)
- Requires comprehensive model cards documenting risks
- Continuous monitoring for performance degradation
- Human oversight for high-stakes decisions

**Outputs**:
- ‚úÖ Model card (Google framework) with performance, fairness, limitations, ethics
- ‚úÖ Continuous monitoring with alerting (AUC drop threshold)
- ‚úÖ SHAP explanations for post-hoc interpretability
- ‚úÖ Deployment decision logic

**Example**:
```python
from healthcare_model_interpretability import MonitoredBlackBoxModel

# Train with baseline AUC for comparison
blackbox = MonitoredBlackBoxModel(baseline_auc=0.797)
blackbox.train(X_train, y_train)

# Deploy only if gain > 20%
is_deployed = blackbox.deploy_with_monitoring(X_test, y_test)

# Explain with SHAP
if is_deployed:
    blackbox.explain_with_shap(X_test, n_samples=100)
```

**Result**: AUC = 0.991 (training), 0.786 (test)
- **AUC Gain**: 24.3% over baseline ‚Üí Deployment justified
- **Monitoring**: Alert triggered when test AUC dropped >5%
- **Decision**: Deployment BLOCKED due to overfitting concerns
- **Action**: Regularize model, increase validation set, retest

---

## üõ†Ô∏è Installation

```bash
# Install dependencies
pip install -r examples/requirements_interpretability.txt

# Or install individually
pip install numpy pandas scikit-learn xgboost shap matplotlib seaborn
```

---

## üöÄ Quick Start

```bash
# Run full demonstration
python examples/healthcare_model_interpretability.py

# Outputs saved to:
# /home/user/GenZ/output/
#   - logreg_coefficients.png
#   - shap_patient_0_waterfall.png
#   - shap_summary_population.png
#   - shap_dependence_*.png
#   - pdp_population.png
#   - subgroup_auc_age_group.png
```

---

## üìà Generated Visualizations

### 1. **Logistic Regression Coefficients**
![Logistic Regression](../output/logreg_coefficients.png)
- Shows feature importance as log-odds ratios
- Negative coefficient = protective factor (e.g., hemoglobin)
- Positive coefficient = risk factor (e.g., comorbidity count)

### 2. **SHAP Waterfall Plot (Individual Patient)**
![SHAP Waterfall](../output/shap_patient_0_waterfall.png)
- Explains prediction for Patient 0
- Shows how each feature pushes prediction up or down
- Base value (average prediction) ‚Üí Final prediction

### 3. **SHAP Summary Plot (Global)**
![SHAP Summary](../output/shap_summary_population.png)
- Beeswarm plot showing feature impact across all patients
- Color = feature value (red=high, blue=low)
- X-axis = SHAP value (impact on prediction)
- Most important features at the top

### 4. **SHAP Dependence Plot**
![SHAP Dependence](../output/shap_dependence_age.png)
- Shows how a single feature affects predictions
- X-axis = feature value
- Y-axis = SHAP value (impact)
- Color = interaction feature (auto-selected)

### 5. **Partial Dependence Plot (PDP)**
![PDP](../output/pdp_population.png)
- Shows marginal effect of features
- Averaged over other features
- Useful for policy: "If we reduce X by 10%, risk drops by Y%"

### 6. **Subgroup Performance Analysis**
![Subgroup AUC](../output/subgroup_auc_age_group.png)
- Model performance across demographic groups
- Identifies equity issues
- Example: 80+ age group has lower AUC ‚Üí needs targeted improvement

---

## üî¨ Key Findings from Synthetic Data

### **Top Risk Factors for Hospital Readmission**

1. **Hemoglobin** (-0.69 coefficient)
   - Lower hemoglobin = higher risk
   - Clinical action: Check for anemia before discharge

2. **Comorbidity Count** (+0.68 coefficient)
   - More chronic conditions = higher risk
   - Clinical action: Intensive care coordination for poly-chronic patients

3. **Age** (+0.66 coefficient)
   - Older patients at higher risk
   - Risk accelerates sharply after age 70 (from PDP)

4. **Prior Admissions** (+0.47 coefficient)
   - Strongest predictor after clinical factors
   - Clinical action: Flag patients with 2+ prior admissions

5. **ICU Stay** (+0.37 coefficient)
   - ICU patients need extra discharge support

### **Equity Findings**

- **Age Group Performance**:
  - <50 years: AUC = 0.89 (excellent)
  - 50-65 years: AUC = 0.79 (good)
  - 65-80 years: AUC = 0.81 (good)
  - **80+ years: AUC = 0.62 (poor)** ‚ö†Ô∏è

**Recommendation**: Retrain model with oversampled elderly patients or develop age-specific models.

---

## üéì When to Use Each Approach

### **Use Logistic Regression if**:
- ‚úÖ Clinicians need to explain individual predictions to patients
- ‚úÖ Regulatory requirement for interpretability (GDPR, FDA)
- ‚úÖ High-stakes decisions (surgery, medication, discharge)
- ‚úÖ Model will be used in court or legal proceedings
- ‚úÖ Small dataset (<10k samples)

### **Use XGBoost with Interpretability Tools if**:
- ‚úÖ Population-level surveillance (e.g., disease outbreak prediction)
- ‚úÖ Policy decisions affecting many people
- ‚úÖ Resource allocation (e.g., where to open clinics)
- ‚úÖ Research and hypothesis generation
- ‚úÖ Medium dataset (10k-1M samples)

### **Use Black-Box Models if**:
- ‚úÖ AUC gain > 20% over interpretable baseline
- ‚úÖ Comprehensive model cards are created
- ‚úÖ Continuous monitoring is in place
- ‚úÖ Human oversight is guaranteed
- ‚úÖ Fail-safe mechanism exists (revert to baseline)
- ‚úÖ Annual recertification process defined
- ‚úÖ Large dataset (>1M samples with good label quality)

---

## üìã Model Card Template

Every deployed model should have a **Model Card** documenting:

1. **Model Details**
   - Model type (e.g., Logistic Regression, XGBoost, Random Forest)
   - Intended use (e.g., "Predict 30-day readmission risk")
   - Training date and version

2. **Performance Metrics**
   - AUC-ROC, precision, recall, F1
   - Calibration curves
   - Performance on validation and test sets

3. **Fairness Metrics**
   - Subgroup performance (age, race, gender, geography, SES)
   - Disparate impact ratio (<1.25 is acceptable)
   - Calibration across subgroups

4. **Limitations**
   - What the model CANNOT do
   - Known failure modes
   - Populations where model underperforms

5. **Ethical Considerations**
   - Potential harms
   - Required human oversight
   - Patient consent requirements
   - Privacy and security measures

See `healthcare_model_interpretability.py` lines 46-110 for implementation.

---

## üîç Advanced Features

### **Monotonic Constraints**
Force model to respect clinical knowledge:

```python
# Age should only increase risk, never decrease
# Blood pressure medication should only decrease risk
monotonic_constraints = {
    'age': 1,           # Positive monotonic
    'bp_medication': -1, # Negative monotonic
    'comorbidity_count': 1
}

model = ClinicalDecisionModel(use_monotonic_gbm=True)
model.train(X, y, monotonic_constraints=monotonic_constraints)
```

### **Continuous Monitoring**
Alert when model performance degrades:

```python
from healthcare_model_interpretability import ModelMonitor

monitor = ModelMonitor(baseline_auc=0.85, alert_threshold=0.05)

# Check weekly performance
current_auc = evaluate_model(current_week_data)
is_healthy = monitor.check_performance(current_auc, datetime.now())

if not is_healthy:
    # Trigger retraining or escalation
    send_alert_to_ml_team()
```

### **Subgroup Fairness Analysis**
Ensure model works equitably across populations:

```python
# Analyze performance by protected characteristics
pop_model.subgroup_analysis(X_test, y_test, 'age_group')
pop_model.subgroup_analysis(X_test, y_test, 'race')
pop_model.subgroup_analysis(X_test, y_test, 'income_level')
pop_model.subgroup_analysis(X_test, y_test, 'rural_urban')

# Require: Max AUC difference across subgroups < 0.10
```

---

## üìö References

### **Interpretability**
- Lundberg & Lee (2017). "A Unified Approach to Interpreting Model Predictions" (SHAP)
- Ribeiro et al. (2016). "Why Should I Trust You?" (LIME)
- Molnar (2022). "Interpretable Machine Learning"

### **Healthcare AI Ethics**
- Obermeyer et al. (2019). "Dissecting racial bias in an algorithm" (Science)
- Rajkomar et al. (2018). "Ensuring Fairness in Machine Learning" (NEJM AI)
- Chen et al. (2020). "Ethical Machine Learning in Healthcare" (Annual Review)

### **Model Cards**
- Mitchell et al. (2019). "Model Cards for Model Reporting" (Google)
- Gebru et al. (2020). "Datasheets for Datasets"

### **Monotonic Constraints**
- XGBoost Documentation: https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html
- Gupta et al. (2016). "Monotonic Calibrated Interpolated Look-Up Tables"

---

## üß™ Testing with Your Own Data

Replace the synthetic data generator with your own data:

```python
# Instead of:
# X, y = generate_synthetic_patient_data(n_samples=1000)

# Use your own data:
import pandas as pd

X = pd.read_csv('your_patient_features.csv')
y = pd.read_csv('your_patient_outcomes.csv')['readmitted']

# Ensure no data leakage
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Run the same workflow
clinical_model = ClinicalDecisionModel()
clinical_model.train(X_train, y_train)
# ... etc
```

---

## üö® Important Warnings

### **DO NOT**:
- ‚ùå Deploy models without human oversight in high-stakes settings
- ‚ùå Use black-box models when interpretable models are sufficient
- ‚ùå Skip subgroup analysis (can hide harmful biases)
- ‚ùå Ignore model monitoring (performance degrades over time)
- ‚ùå Use models outside their intended population
- ‚ùå Deploy without patient consent and explanation

### **DO**:
- ‚úÖ Validate models across diverse patient populations
- ‚úÖ Document all modeling decisions (model cards)
- ‚úÖ Monitor continuously for drift and bias
- ‚úÖ Involve clinicians in feature selection and validation
- ‚úÖ Provide explanations to patients when asked
- ‚úÖ Have escalation protocols for anomalies
- ‚úÖ Retrain regularly as clinical practices evolve

---

## üìû Support

For questions or issues:
- GitHub: https://github.com/sechan9999/GenZ/issues
- Email: sechan9999@gmail.com

---

## üìú License

MIT License - see LICENSE file for details

---

**Last Updated**: 2025-11-22
**Version**: 1.0.0
**Author**: Gen Z Agent Team
