# ==========================================
# Retry Queue Configuration
# ==========================================
# This file configures the exponential backoff retry queue system

# Delta Lake table paths
paths:
  # Retry queue table - stores failed records waiting for retry
  retry_queue: "/mnt/delta/lims_retry_queue"

  # Final results table - stores successfully processed records
  final_results: "/mnt/delta/lims_verified_results"

  # Dead Letter Queue - stores permanently failed records for manual review
  dead_letter_queue: "/mnt/delta/lims_dead_letter_queue"

# Retry behavior settings
retry:
  # Maximum number of retry attempts before moving to DLQ
  # After this many retries, records are considered permanently failed
  max_retries: 5

  # Base backoff time in minutes
  # Actual backoff = base * (2 ^ retry_count)
  # Example with base=2: 2min, 4min, 8min, 16min, 32min
  base_backoff_minutes: 2

# Spark configuration
spark:
  # Application name for Spark UI
  app_name: "LIMS_RetryQueue_Processor"

  # Delta Lake extensions (required)
  extensions:
    - "io.delta.sql.DeltaSparkSessionExtension"

  # Catalog configuration (required for Delta)
  catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"

# Processing behavior
processing:
  # Batch size for reading from retry queue
  # Larger batches = better throughput but higher memory
  batch_size: 1000

  # Whether to use parallel processing (mapPartitions)
  # Set to false for small datasets or when order matters
  parallel_processing: true

  # Number of partitions for parallel processing
  num_partitions: 4

# Logging configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: "INFO"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Write logs to file (optional)
  file:
    enabled: true
    path: "/var/log/retry_queue/app.log"
    max_bytes: 10485760  # 10MB
    backup_count: 5

# External system integration settings
external_api:
  # API endpoint for record processing
  endpoint: "https://api.example.com/lims/submit"

  # Request timeout in seconds
  timeout: 30

  # Connection pool settings
  pool_connections: 10
  pool_maxsize: 20

  # Retry settings for API calls (separate from queue retries)
  api_max_retries: 3
  api_backoff_factor: 0.5

# Monitoring and alerts
monitoring:
  # Enable metrics collection
  enabled: true

  # Prometheus pushgateway (if using)
  prometheus:
    enabled: false
    pushgateway_url: "http://localhost:9091"

  # Alert thresholds
  alerts:
    # Alert if DLQ size exceeds this number
    dlq_threshold: 100

    # Alert if retry queue size exceeds this number
    retry_queue_threshold: 1000

    # Alert if success rate drops below this percentage
    min_success_rate: 80

# Scheduling (for use with Airflow, Databricks Jobs, etc.)
schedule:
  # Cron expression for job scheduling
  # Every 5 minutes: "*/5 * * * *"
  cron: "*/5 * * * *"

  # Job timeout in minutes
  timeout_minutes: 30

  # Maximum concurrent runs
  max_concurrent_runs: 1
